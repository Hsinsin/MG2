---
license: mit
---

## Model Overview
The **Melody Guided Music Generation (MGÂ²)** model is an innovative approach that uses melody to guide music generation, achieving impressive results despite its simplicity and minimal resource requirements. MGÂ² aligns melody with audio waveforms and text descriptions via a multimodal alignment module and conditions its diffusion module on these learned melody representations. This enables MGÂ² to create music that matches the style of given audio and reflects the content of text descriptions.

## Demo
Explore the capabilities of the MGÂ² model through an online demo:
- **Demo Link**: [Model Demo](https://awesome-mmgen.github.io/)
- **Instructions**: Input a text description, then click "Generate" to see the music generated by the model.

## GitHub Repository
Access the code and additional resources for the MGÂ² model:
- **GitHub Link**: [MGÂ² on GitHub](https://github.com/shaopengw/Awesome-Music-Generation)

## Integration with Transformers and Hugging Face Hub
We are currently working on integrating MGÂ² into the **Hugging Face Transformers** library and making it available on the **Hugging Face Hub** ðŸ¤—.

### Tips: To generate high-quality music using MGÂ², you'd better craft detailed and descriptive prompts that provide rich context and specific musical elements.

## Paper
> **Title**: "Melody Is All You Need For Music Generation" Â 
> **Authors**: Shaopeng Wei, Manzhen Wei, Haoyu Wang, Yu Zhao, Gang Kou Â 
> **Year**: 2024 Â 
> [arXiv Link](https://arxiv.org/abs/2409.20196) Â 


## Citation
```bibtex
@article{wei2024melodyneedmusicgeneration,
Â  Â  Â  title={Melody Is All You Need For Music Generation}, 
Â  Â  Â  author={Shaopeng Wei and Manzhen Wei and Haoyu Wang and Yu Zhao and Gang Kou},
Â  Â  Â  year={2024},
Â  Â  Â  eprint={2409.20196},
Â  Â  Â  archivePrefix={arXiv},
Â  Â  Â  primaryClass={cs.SD},
Â  Â  Â  url={https://arxiv.org/abs/2409.20196}, 
}